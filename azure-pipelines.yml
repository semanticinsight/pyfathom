# Python package
# Create and test a Python package on multiple Python versions.
# Add steps that analyze code, save the dist with the build record, publish to a PyPI-compatible index, and more:
# https://docs.microsoft.com/azure/devops/pipelines/languages/python

variables:
- group: databricks
- group: data-platform-kv
- group: fathom

trigger:
- main

pool:
  vmImage: 'ubuntu-latest'
strategy:
  matrix:
    Python37:
      python.version: '3.7'

steps:
- task: UsePythonVersion@0
  inputs:
    versionSpec: '$(python.version)'
  displayName: 'Use Python $(python.version)'

- script: |
    python -m pip install --upgrade pip setuptools wheel twine
    pip install -r requirements.txt
  displayName: 'Install dependencies'

- script: |
    echo "y
    $(WORKSPACE-REGION-URL)
    $(DATABRICKS-PAT-TOKEN)
    $(TEST-CLUSTER-ID)
    $(WORKSPACE-ORG-ID)
    15001" | databricks-connect configure
  displayName: 'Configure DBConnect'

- script: |
    python setup.py sdist bdist_wheel
    ls dist/
  displayName: 'Artifact creation'

- script: |
    pip install --editable .
    pip install pytest pytest-azurepipelines
    pytest
  displayName: 'Unit Tests'

- task: CopyFiles@2
  inputs:
    SourceFolder: '$(Build.SourcesDirectory)/dist'
    Contents: '**'
    TargetFolder: '$(Build.ArtifactStagingDirectory)'

- task: PublishBuildArtifacts@1
  inputs:
    PathtoPublish: '$(Build.ArtifactStagingDirectory)'
    ArtifactName: 'dist'
    publishLocation: 'Container'
  displayName: 'Publish Build Artefacts'

- task: TwineAuthenticate@0
  inputs:
    artifactFeeds: 'sibytes'
    # externalFeeds: 'pypi'
  displayName: 'Authenticate Twine'

- script: |
    twine upload -r sibytes --config-file $(PYPIRC_PATH) $(Build.SourcesDirectory)/dist/*
  continueOnError: true
  displayName: 'Publish to SiBytes Artefact Store'